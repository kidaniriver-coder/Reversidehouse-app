import os
import streamlit as st

from cli.loader import load_documents_to_chunks, read_texts_with_names
from cli.retrieval import ChunkRetriever
from cli.dialogue import CLIDialogueEngine

# Optional LLM
LLM_AVAILABLE = True
try:
	from llm.engine import LLMEngine
except Exception:
	LLM_AVAILABLE = False

st.set_page_config(page_title="Minpaku Chatbot", page_icon="ğŸ ", layout="centered")

st.title("æ°‘æ³Šã‚²ã‚¹ãƒˆå‘ã‘ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (MVP)")

# Sidebar: data source + options
project_root = os.path.dirname(__file__)
default_documents = os.path.join(project_root, "documents")

doc_dir = st.sidebar.text_input("PDFãƒ•ã‚©ãƒ«ãƒ€ (documents)", value=default_documents)
use_llm = st.sidebar.checkbox("LLMã§å›ç­”ç”Ÿæˆã™ã‚‹ (RAG)", value=LLM_AVAILABLE, disabled=not LLM_AVAILABLE)
model_name = st.sidebar.text_input("OpenAIãƒ¢ãƒ‡ãƒ«", value="gpt-4o-mini", disabled=not use_llm)
reload_clicked = st.sidebar.button("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†èª­è¾¼")

with st.expander("è¨ºæ–­æƒ…å ±"):
	try:
		import httpx  # type: ignore
		import openai  # type: ignore
		st.write({"httpx": getattr(httpx, "__version__", "unknown"), "openai": getattr(openai, "__version__", "unknown")})
	except Exception as e:
		st.write({"error": str(e)})
	st.write({"LLM_AVAILABLE": LLM_AVAILABLE})
	if "chunks" in st.session_state:
		st.write({"chunks": len(st.session_state.get("chunks", []))})
		st.write({"files": st.session_state.get("loaded_files", [])})

if "retriever" not in st.session_state or reload_clicked or st.session_state.get("doc_dir") != doc_dir:
	# Load corpus (TXTå„ªå…ˆã€ãªã‘ã‚Œã°PDF)
	chunks = load_documents_to_chunks(doc_dir)
	texts_with_names = read_texts_with_names(doc_dir)
	st.session_state.loaded_files = [name for name, _ in texts_with_names]

	st.session_state.doc_dir = doc_dir
	st.session_state.chunks = chunks
	st.session_state.retriever = ChunkRetriever(chunks)
	st.session_state.engine = CLIDialogueEngine(st.session_state.retriever)
	st.session_state.messages = []
	if not chunks:
		st.info("documentså†…ã®TXTã¾ãŸã¯PDFãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ã¨æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’ç¢ºèªã—ã€å†èª­è¾¼ã—ã¦ãã ã•ã„ã€‚")

if use_llm and LLM_AVAILABLE:
	try:
		st.session_state.llm = LLMEngine(model=model_name)
	except Exception as e:
		st.warning(f"LLMåˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
		st.session_state.llm = None
else:
	st.session_state.llm = None

# Chat history
if "messages" not in st.session_state:
	st.session_state.messages = []

for m in st.session_state.messages:
	with st.chat_message(m["role"]):
		st.markdown(m["content"])

user_input = st.chat_input("ã”è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
if user_input:
	st.session_state.messages.append({"role": "user", "content": user_input})
	with st.chat_message("user"):
		st.markdown(user_input)

	assistant_text = ""
	if st.session_state.llm is not None:
		retrieved = st.session_state.retriever.search(user_input, top_k=5)
		context_chunks = [text for text, _ in retrieved]
		try:
			assistant_text = st.session_state.llm.generate(user_input, context_chunks)
		except Exception as e:
			assistant_text = f"LLMå‘¼ã³å‡ºã—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}"
	else:
		decision = st.session_state.engine.handle(user_input)
		assistant_text = decision.get("text") or "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
		options = decision.get("options")
		if options:
			assistant_text += "\n\nå€™è£œ:\n" + "\n".join([f"- {o}" for o in options])

	with st.chat_message("assistant"):
		st.markdown(assistant_text)
	st.session_state.messages.append({"role": "assistant", "content": assistant_text})
